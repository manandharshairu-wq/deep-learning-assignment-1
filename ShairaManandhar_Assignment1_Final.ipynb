{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7qrHGj6c5BxMsX4/C9gkE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manandharshairu-wq/deep-learning-assignment-1/blob/main/ShairaManandhar_Assignment1_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UM6YVzH2E4S"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0) MOUNT DRIVE + SETUP\n",
        "# =========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, json, torch\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/shairamanandhar_assignment1_final\"\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "%cd $base_path\n",
        "\n",
        "# Project structure\n",
        "for folder in [\"models\", \"data_loaders\", \"config\", \"results\", \"data\"]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(\"Project at:\", base_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install deps\n",
        "!pip -q install torch torchvision numpy pandas scikit-learn matplotlib tqdm tensorflow-datasets"
      ],
      "metadata": {
        "id": "LhjwTW9B2xaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFDS cache (PCam uses TFDS)\n",
        "import os\n",
        "os.environ[\"TFDS_DATA_DIR\"] = \"/content/tfds_cache\"\n",
        "os.makedirs(os.environ[\"TFDS_DATA_DIR\"], exist_ok=True)\n",
        "print(\"TFDS cache:\", os.environ[\"TFDS_DATA_DIR\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11QBmTok2xZj",
        "outputId": "d831d7d9-8d69-4466-d552-0e122ad77aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFDS cache: /content/tfds_cache\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1) CONFIG FILE\n",
        "# =========================\n",
        "import torch\n",
        "\n",
        "config = {\n",
        "    \"seed\": 42,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    # Training (consistent across all models)\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 128,\n",
        "    \"epochs\": 12,\n",
        "\n",
        "    # Early stopping (consistent)\n",
        "    \"early_stopping\": {\n",
        "        \"enabled\": True,\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 3,\n",
        "        \"min_delta\": 0.0\n",
        "    },\n",
        "\n",
        "    # What to run (config-driven)\n",
        "    # NOTE: PCam is implemented but not run by default\n",
        "    \"run_datasets\": [\"Adult\", \"CIFAR-100(0-9)\"],\n",
        "\n",
        "    # \"ViT\" means:\n",
        "    # - Adult -> TabularAttention\n",
        "    # - CIFAR/PCam -> TinyViT\n",
        "    \"run_architectures\": [\"MLP\", \"CNN\", \"ViT\"],\n",
        "\n",
        "    # Splits\n",
        "    \"adult_test_size\": 0.2,\n",
        "    \"adult_val_size\": 0.15,\n",
        "    \"val_fraction\": 0.15,\n",
        "\n",
        "    # PCam limits (for when you do try it)\n",
        "    \"pcam_train_limit\": 10000,\n",
        "    \"pcam_val_limit\": 2000,\n",
        "    \"pcam_test_limit\": 2000,\n",
        "\n",
        "    # Arch 3 configs\n",
        "    \"tabattn\": {\"d_model\": 64, \"heads\": 4, \"layers\": 2, \"dropout\": 0.1},\n",
        "    \"vit\": {\"patch\": 4, \"dim\": 128, \"heads\": 4, \"layers\": 2, \"dropout\": 0.1},\n",
        "\n",
        "    # Bonus\n",
        "    \"bonus\": {\n",
        "        \"make_learning_curve_comparison\": True,\n",
        "        \"save_param_counts\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"config/config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Saved config/config.json\")\n",
        "print(\"Device detected:\", config[\"device\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktWRsFVu2xVz",
        "outputId": "41fb6e26-524a-4a1c-b8a7-6e64cbd890f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved config/config.json\n",
            "Device detected: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) MODELS (architectures.py)\n",
        "# =========================\n",
        "%%writefile models/architectures.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Architecture 1: MLP\n",
        "# -------------------------\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Mandatory MLP:\n",
        "    Input -> 2 hidden layers (ReLU) -> Output\n",
        "    Includes BatchNorm + Dropout\n",
        "    Works for:\n",
        "      - Adult tabular (B,D)\n",
        "      - Images (B,C,H,W) after flatten\n",
        "    \"\"\"\n",
        "    def __init__(self, in_shape, out_dim, hidden1=512, hidden2=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "        flat_size = in_shape if isinstance(in_shape, int) else int(np.prod(in_shape))\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flat_size, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden1),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden2),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(hidden2, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -------------------------\n",
        "# Architecture 2: CNN\n",
        "# -------------------------\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Mandatory CNN:\n",
        "    At least 2 convolution layers + pooling + FC head.\n",
        "    Works for:\n",
        "      - Adult: Conv1D over features\n",
        "      - Images: Conv2D\n",
        "    \"\"\"\n",
        "    def __init__(self, in_shape, out_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(in_shape, int):\n",
        "            # Tabular as 1D signal: [B, 1, D]\n",
        "            self.feat = nn.Sequential(\n",
        "                nn.Unflatten(1, (1, in_shape)),\n",
        "                nn.Conv1d(1, 16, 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(2),\n",
        "                nn.Conv1d(16, 32, 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(2),\n",
        "                nn.Flatten()\n",
        "            )\n",
        "            L = in_shape // 4  # pooled twice\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(32 * L, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(128, out_dim)\n",
        "            )\n",
        "        else:\n",
        "            # Image CNN: [B, C, H, W]\n",
        "            C, H, W = in_shape\n",
        "            self.feat = nn.Sequential(\n",
        "                nn.Conv2d(C, 32, 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(32, 64, 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Flatten()\n",
        "            )\n",
        "            H2, W2 = H // 4, W // 4\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(64 * H2 * W2, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(256, out_dim)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.feat(x))\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Architecture 3a (Bonus): Tabular Transformer-style Attention\n",
        "# Used for Adult\n",
        "# --------------------------------------------------------\n",
        "class TabularAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Treat each tabular feature as a token.\n",
        "    - Feature scalar -> token embedding\n",
        "    - Transformer encoder\n",
        "    - Mean pool tokens -> classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, out_dim, d_model=64, n_heads=4, n_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.feature_embed = nn.Linear(1, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_features, d_model))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, D]\n",
        "        x = x.unsqueeze(-1)              # [B, D, 1]\n",
        "        x = self.feature_embed(x)        # [B, D, d_model]\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]\n",
        "        x = self.encoder(x)              # [B, D, d_model]\n",
        "        x = self.norm(x)\n",
        "        x = x.mean(dim=1)                # mean pool over features\n",
        "        return self.head(x)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Architecture 3b (Bonus): Vision Transformer-style (TinyViT)\n",
        "# Used for CIFAR + PCam\n",
        "# --------------------------------------------------------\n",
        "class TinyViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple ViT-style encoder for images only.\n",
        "    No pretrained weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_shape, out_dim, patch=4, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert not isinstance(in_shape, int), \"TinyViT is for images only.\"\n",
        "\n",
        "        C, H, W = in_shape\n",
        "        assert H % patch == 0 and W % patch == 0, \"H and W must be divisible by patch size.\"\n",
        "        n_patches = (H // patch) * (W // patch)\n",
        "\n",
        "        self.patch_embed = nn.Conv2d(C, d_model, kernel_size=patch, stride=patch)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + n_patches, d_model))\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,C,H,W]\n",
        "        x = self.patch_embed(x)          # [B,d,H/p,W/p]\n",
        "        x = x.flatten(2).transpose(1, 2) # [B,tokens,d]\n",
        "        B = x.size(0)\n",
        "\n",
        "        cls = self.cls_token.expand(B, -1, -1)  # [B,1,d]\n",
        "        x = torch.cat([cls, x], dim=1)          # [B,1+tokens,d]\n",
        "        x = self.drop(x + self.pos_embed[:, :x.size(1), :])\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        cls_out = self.norm(x[:, 0])    # CLS token\n",
        "        return self.head(cls_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9IPDy1i2xP7",
        "outputId": "8dee9ece-fe75-43d1-a543-2e823a8a8822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models/architectures.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) DATA LOADERS (loaders.py)\n",
        "# =========================\n",
        "%%writefile data_loaders/loaders.py\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, Dataset, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# -----------------------\n",
        "# Adult (tabular) - binary\n",
        "# -----------------------\n",
        "def get_adult(seed=42, test_size=0.2, val_size=0.15):\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "    cols = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "            'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "            'hours-per-week', 'native-country', 'income']\n",
        "    df = pd.read_csv(url, names=cols, skipinitialspace=True).dropna()\n",
        "\n",
        "    # Label\n",
        "    df['income'] = df['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
        "\n",
        "    # Encode categoricals\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    X = StandardScaler().fit_transform(df.drop('income', axis=1).values)\n",
        "    y = df['income'].values\n",
        "\n",
        "    # Train/test then train/val\n",
        "    X_tr_val, X_te, y_tr_val, y_te = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "        X_tr_val, y_tr_val, test_size=val_size, random_state=seed, stratify=y_tr_val\n",
        "    )\n",
        "\n",
        "    train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr).view(-1, 1))\n",
        "    val_ds   = TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(y_va).view(-1, 1))\n",
        "    test_ds  = TensorDataset(torch.FloatTensor(X_te), torch.FloatTensor(y_te).view(-1, 1))\n",
        "\n",
        "    return train_ds, val_ds, test_ds, X.shape[1], 1, \"binary\"\n",
        "\n",
        "# -----------------------------------------\n",
        "# CIFAR-100 but only classes 0–9 (10-class)\n",
        "# -----------------------------------------\n",
        "def get_cifar100_10class(val_fraction=0.15, seed=42):\n",
        "    t = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "    full = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=t)\n",
        "    test_full = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=t)\n",
        "\n",
        "    # Keep labels 0..9 only\n",
        "    train_idx = [i for i, y in enumerate(full.targets) if y in range(10)]\n",
        "    test_idx  = [i for i, y in enumerate(test_full.targets) if y in range(10)]\n",
        "\n",
        "    full_10 = Subset(full, train_idx)\n",
        "    test_10 = Subset(test_full, test_idx)\n",
        "\n",
        "    # Split train/val\n",
        "    v_sz = int(len(full_10) * val_fraction)\n",
        "    tr_sz = len(full_10) - v_sz\n",
        "    train, val = random_split(full_10, [tr_sz, v_sz], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "    return train, val, test_10, (3, 32, 32), 10, \"multiclass\"\n",
        "\n",
        "# -----------------------\n",
        "# PCam (TFDS) - binary\n",
        "# -----------------------\n",
        "class PCamTorch(Dataset):\n",
        "    def __init__(self, split=\"train\", limit=None):\n",
        "        ds = tfds.load(\n",
        "            \"patch_camelyon\",\n",
        "            split=split,\n",
        "            as_supervised=True,\n",
        "            data_dir=os.environ.get(\"TFDS_DATA_DIR\")\n",
        "        )\n",
        "        if limit is not None:\n",
        "            ds = ds.take(limit)\n",
        "        self.samples = list(tfds.as_numpy(ds))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, y = self.samples[idx]\n",
        "        img = torch.tensor(img, dtype=torch.float32) / 255.0\n",
        "        img = img.permute(2, 0, 1)  # CHW\n",
        "        y = torch.tensor(y, dtype=torch.float32).view(1)\n",
        "        return img, y\n",
        "\n",
        "def get_pcam(limit_train=10000, limit_val=2000, limit_test=2000):\n",
        "    train = PCamTorch(split=\"train\", limit=limit_train)\n",
        "    val   = PCamTorch(split=\"validation\", limit=limit_val)\n",
        "    test  = PCamTorch(split=\"test\", limit=limit_test)\n",
        "    return train, val, test, (3, 96, 96), 1, \"binary\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ4hvX_v2xKV",
        "outputId": "3525124d-4e1a-4f1d-a76e-0a1c7f5140f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_loaders/loaders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4) MAIN TRAINING SCRIPT (main.py)\n",
        "# =========================\n",
        "%%writefile main.py\n",
        "import os, json, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from data_loaders.loaders import get_adult, get_cifar100_10class, get_pcam\n",
        "from models.architectures import MLP, CNN, TinyViT, TabularAttention\n",
        "\n",
        "with open(\"config/config.json\", \"r\") as f:\n",
        "    CFG = json.load(f)\n",
        "\n",
        "DEVICE = CFG[\"device\"]\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def make_loaders(train_ds, val_ds, test_ds):\n",
        "    pin = (DEVICE == \"cuda\")\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,  num_workers=0, pin_memory=pin),\n",
        "        DataLoader(val_ds,   batch_size=CFG[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=pin),\n",
        "        DataLoader(test_ds,  batch_size=CFG[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=pin),\n",
        "    )\n",
        "\n",
        "def criterion_for(task):\n",
        "    return nn.BCEWithLogitsLoss() if task == \"binary\" else nn.CrossEntropyLoss()\n",
        "\n",
        "def metric_from_logits(logits, y, task):\n",
        "    # consistent metric reporting\n",
        "    if task == \"binary\":\n",
        "        probs = torch.sigmoid(logits.view(-1))\n",
        "        preds = (probs >= 0.5).long().cpu().numpy()\n",
        "        labels = y.view(-1).long().cpu().numpy()\n",
        "        return f1_score(labels, preds, average=\"binary\")\n",
        "    else:\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        labels = y.cpu().numpy()\n",
        "        return accuracy_score(labels, preds)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_metrics(model, test_loader, task):\n",
        "    model.eval()\n",
        "    all_logits, all_y = [], []\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        all_logits.append(model(x).cpu())\n",
        "        all_y.append(y.cpu())\n",
        "    logits = torch.cat(all_logits)\n",
        "    y = torch.cat(all_y)\n",
        "\n",
        "    if task == \"binary\":\n",
        "        p = (torch.sigmoid(logits.view(-1)) >= 0.5).long().numpy()\n",
        "        l = y.view(-1).long().numpy()\n",
        "        return accuracy_score(l, p), f1_score(l, p, average=\"binary\")\n",
        "    else:\n",
        "        p = logits.argmax(1).numpy()\n",
        "        l = y.numpy()\n",
        "        return accuracy_score(l, p), f1_score(l, p, average=\"weighted\")\n",
        "\n",
        "def save_curves(history, name, metric_name):\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.title(f\"{name} Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"results/{name}_loss_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"train_metric\"], label=f\"Train {metric_name}\")\n",
        "    plt.plot(history[\"val_metric\"], label=f\"Val {metric_name}\")\n",
        "    plt.title(f\"{name} {metric_name}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"results/{name}_{metric_name.lower()}_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "def train_eval(model, train_loader, val_loader, task):\n",
        "    # optimizer family consistent\n",
        "    assert CFG[\"optimizer\"].lower() == \"adam\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "    crit = criterion_for(task)\n",
        "\n",
        "    es = CFG[\"early_stopping\"]\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    bad = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_metric\": [], \"val_metric\": []}\n",
        "\n",
        "    for epoch in range(CFG[\"epochs\"]):\n",
        "        # ---- train\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        tr_logits, tr_y = [], []\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "\n",
        "            if task == \"binary\":\n",
        "                loss = crit(logits.view(-1, 1), y.float().view(-1, 1))\n",
        "            else:\n",
        "                loss = crit(logits, y.long())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_sum += loss.item()\n",
        "            tr_logits.append(logits.detach().cpu())\n",
        "            tr_y.append(y.detach().cpu())\n",
        "\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "        train_metric = metric_from_logits(torch.cat(tr_logits), torch.cat(tr_y), task)\n",
        "\n",
        "        # ---- val\n",
        "        model.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        va_logits, va_y = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(x)\n",
        "\n",
        "                if task == \"binary\":\n",
        "                    loss = crit(logits.view(-1, 1), y.float().view(-1, 1))\n",
        "                else:\n",
        "                    loss = crit(logits, y.long())\n",
        "\n",
        "                val_loss_sum += loss.item()\n",
        "                va_logits.append(logits.detach().cpu())\n",
        "                va_y.append(y.detach().cpu())\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        val_metric = metric_from_logits(torch.cat(va_logits), torch.cat(va_y), task)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_metric\"].append(train_metric)\n",
        "        history[\"val_metric\"].append(val_metric)\n",
        "\n",
        "        # early stopping on val loss\n",
        "        improved = (best_val - val_loss) > es.get(\"min_delta\", 0.0)\n",
        "        if improved:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "            best_epoch = epoch + 1\n",
        "        else:\n",
        "            bad += 1\n",
        "            if es[\"enabled\"] and bad >= es[\"patience\"]:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return history, best_epoch\n",
        "\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name == \"Adult\":\n",
        "        return get_adult(CFG[\"seed\"], CFG[\"adult_test_size\"], CFG[\"adult_val_size\"])\n",
        "    elif dataset_name == \"CIFAR-100(0-9)\":\n",
        "        return get_cifar100_10class(CFG[\"val_fraction\"], CFG[\"seed\"])\n",
        "    else:\n",
        "        return get_pcam(CFG[\"pcam_train_limit\"], CFG[\"pcam_val_limit\"], CFG[\"pcam_test_limit\"])\n",
        "\n",
        "def build_model(arch, dataset_name, in_shape, out_dim):\n",
        "    if arch == \"MLP\":\n",
        "        return MLP(in_shape, out_dim)\n",
        "\n",
        "    if arch == \"CNN\":\n",
        "        return CNN(in_shape, out_dim)\n",
        "\n",
        "    # Arch 3 (Bonus)\n",
        "    # - Adult: TabularAttention\n",
        "    # - CIFAR/PCam: TinyViT\n",
        "    if arch == \"ViT\":\n",
        "        if dataset_name == \"Adult\":\n",
        "            tcfg = CFG[\"tabattn\"]\n",
        "            return TabularAttention(\n",
        "                num_features=in_shape,\n",
        "                out_dim=out_dim,\n",
        "                d_model=tcfg[\"d_model\"],\n",
        "                n_heads=tcfg[\"heads\"],\n",
        "                n_layers=tcfg[\"layers\"],\n",
        "                dropout=tcfg[\"dropout\"]\n",
        "            )\n",
        "        else:\n",
        "            vcfg = CFG[\"vit\"]\n",
        "            return TinyViT(\n",
        "                in_shape, out_dim,\n",
        "                patch=vcfg[\"patch\"],\n",
        "                d_model=vcfg[\"dim\"],\n",
        "                n_heads=vcfg[\"heads\"],\n",
        "                n_layers=vcfg[\"layers\"],\n",
        "                dropout=vcfg[\"dropout\"]\n",
        "            )\n",
        "\n",
        "    raise ValueError(f\"Unknown architecture: {arch}\")\n",
        "\n",
        "def run_experiment(dataset_name, arch):\n",
        "    train_ds, val_ds, test_ds, in_shape, out_dim, task = load_dataset(dataset_name)\n",
        "    train_loader, val_loader, test_loader = make_loaders(train_ds, val_ds, test_ds)\n",
        "\n",
        "    model = build_model(arch, dataset_name, in_shape, out_dim).to(DEVICE)\n",
        "    params = count_params(model)\n",
        "\n",
        "    start = time.time()\n",
        "    history, best_epoch = train_eval(model, train_loader, val_loader, task)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    acc, f1 = test_metrics(model, test_loader, task)\n",
        "\n",
        "    # Friendly naming: show Adult + ViT as TabularAttention in results table\n",
        "    shown_arch = \"TabularAttention\" if (arch == \"ViT\" and dataset_name == \"Adult\") else arch\n",
        "\n",
        "    exp_name = f\"{dataset_name}_{shown_arch}\".replace(\"/\", \"_\")\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    torch.save(model.state_dict(), f\"results/{exp_name}_best.pt\")\n",
        "\n",
        "    metric_name = \"F1\" if task == \"binary\" else \"Accuracy\"\n",
        "    save_curves(history, exp_name, metric_name)\n",
        "\n",
        "    notes = f\"time={train_time:.1f}s; best_epoch={best_epoch}; params={params}\"\n",
        "    return {\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Architecture\": shown_arch,\n",
        "        \"Accuracy\": round(acc, 4),\n",
        "        \"F1\": round(f1, 4),\n",
        "        \"Notes\": notes,\n",
        "        \"_history\": history\n",
        "    }\n",
        "\n",
        "def plot_compare(histories, title, out_path, key):\n",
        "    plt.figure()\n",
        "    for label, hist in histories.items():\n",
        "        plt.plot(hist[key], label=label)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(CFG[\"seed\"])\n",
        "    print(\"Using device:\", DEVICE)\n",
        "\n",
        "    run_datasets = CFG[\"run_datasets\"]\n",
        "    run_archs = CFG[\"run_architectures\"]\n",
        "\n",
        "    results = []\n",
        "    histories_by_dataset = {}\n",
        "\n",
        "    for d in run_datasets:\n",
        "        histories_by_dataset[d] = {}\n",
        "        for a in run_archs:\n",
        "            print(f\"Running: {d} + {a}\")\n",
        "            r = run_experiment(d, a)\n",
        "            histories_by_dataset[d][r[\"Architecture\"]] = r[\"_history\"]\n",
        "            results.append({k: r[k] for k in [\"Dataset\", \"Architecture\", \"Accuracy\", \"F1\", \"Notes\"]})\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"results/final_metrics.csv\", index=False)\n",
        "\n",
        "    print(\"\\nFinal Results Table:\\n\")\n",
        "    print(df.to_markdown(index=False))\n",
        "\n",
        "    # Bonus: per-dataset learning curve comparisons\n",
        "    if CFG.get(\"bonus\", {}).get(\"make_learning_curve_comparison\", False):\n",
        "        for d, hdict in histories_by_dataset.items():\n",
        "            if len(hdict) <= 1:\n",
        "                continue\n",
        "            plot_compare(hdict, f\"{d} - Val Loss Comparison\", f\"results/{d}_VAL_LOSS_COMPARISON.png\", \"val_loss\")\n",
        "            plot_compare(hdict, f\"{d} - Val Metric Comparison\", f\"results/{d}_VAL_METRIC_COMPARISON.png\", \"val_metric\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp6e_eX32w_l",
        "outputId": "e22b83b5-e98f-4a47-a05d-462ab106dc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5gToScw33Kf",
        "outputId": "49d93344-64b9-4a27-93be-a7251bab9170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Running: Adult + MLP\n",
            "Running: Adult + CNN\n",
            "Running: Adult + ViT\n",
            "Running: CIFAR-100(0-9) + MLP\n",
            "100% 169M/169M [00:03<00:00, 43.7MB/s]\n",
            "Running: CIFAR-100(0-9) + CNN\n",
            "Running: CIFAR-100(0-9) + ViT\n",
            "\n",
            "Final Results Table:\n",
            "\n",
            "| Dataset        | Architecture     |   Accuracy |     F1 | Notes                                     |\n",
            "|:---------------|:-----------------|-----------:|-------:|:------------------------------------------|\n",
            "| Adult          | MLP              |     0.8535 | 0.6669 | time=8.5s; best_epoch=12; params=140801   |\n",
            "| Adult          | CNN              |     0.8548 | 0.6702 | time=9.0s; best_epoch=10; params=14177    |\n",
            "| Adult          | TabularAttention |     0.8511 | 0.6769 | time=14.7s; best_epoch=7; params=105345   |\n",
            "| CIFAR-100(0-9) | MLP              |     0.555  | 0.5541 | time=16.7s; best_epoch=8; params=1708810  |\n",
            "| CIFAR-100(0-9) | CNN              |     0.69   | 0.6884 | time=17.4s; best_epoch=11; params=1070794 |\n",
            "| CIFAR-100(0-9) | ViT              |     0.623  | 0.6266 | time=21.2s; best_epoch=12; params=412810  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "\n",
        "# Deep Learning Assignment\n",
        "## Shaira Manandhar\n",
        "**Datasets × Architectures Benchmark (PyTorch)**\n",
        "\n",
        "## Objective\n",
        "This project benchmarks how different neural network architectures perform across tabular and image datasets, highlighting how data modality and model inductive bias affect learning.\n",
        "All models are trained from scratch (no pretraining) with a consistent experimental setup.\n",
        "\n",
        "---\n",
        "\n",
        "## Datasets\n",
        "- **Adult Income (UCI)**\n",
        "  - Modality: Tabular\n",
        "  - Task: Binary classification (income >50K)\n",
        "  - Metrics: Accuracy, F1-score\n",
        "\n",
        "- **CIFAR-100 (classes 0–9)**\n",
        "  - Modality: Image (32×32 RGB)\n",
        "  - Task: 10-class classification\n",
        "  - Metric: Accuracy\n",
        "\n",
        "- **PatchCamelyon (PCam)** *(implemented, not run by default)*\n",
        "  - Modality: Histopathology images (96×96 RGB)\n",
        "  - Task: Binary classification (tumor vs normal)\n",
        "\n",
        "---\n",
        "\n",
        "## Architectures\n",
        "### 1. Multilayer Perceptron (MLP)\n",
        "- Fully connected feedforward network\n",
        "- 2 hidden layers (ReLU)\n",
        "- Batch Normalization + Dropout\n",
        "- Used on **all datasets** (after preprocessing)\n",
        "\n",
        "### 2. Convolutional Neural Network (CNN)\n",
        "- ≥2 convolution layers with pooling\n",
        "- Conv1D for tabular data, Conv2D for images\n",
        "- Fully connected classifier head\n",
        "- Used on **all datasets**\n",
        "\n",
        "### 3. Attention-Based Model\n",
        "- **Adult**: Transformer-style Tabular Attention (features treated as tokens)\n",
        "- **CIFAR / PCam**: Vision Transformer (TinyViT)\n",
        "- No pretrained weights\n",
        "\n",
        "---\n",
        "\n",
        "## Experimental Setup\n",
        "- Framework: PyTorch\n",
        "- Optimizer: Adam (same across all models)\n",
        "- Batch size: 128\n",
        "- Learning rate: 0.001\n",
        "- Epochs: 12\n",
        "- Early stopping on validation loss (patience = 3)\n",
        "- Train / validation / test splits are consistent per dataset\n",
        "\n",
        "---\n",
        "##Note: PCam is implemented but not run by default due to compute constraints in Colab.\n",
        "\n",
        "---\n",
        "\n",
        "## Results Summary\n",
        "| Dataset         | Architecture     | Accuracy | F1 |\n",
        "|-----------------|------------------|----------|----|\n",
        "| Adult           | MLP              | 0.8535   | 0.6669 |\n",
        "| Adult           | CNN              | 0.8548   | 0.6702 |\n",
        "| Adult           | TabularAttention | 0.8511   | 0.6769 |\n",
        "| CIFAR-100 (0–9) | MLP              | 0.5550   | 0.5541 |\n",
        "| CIFAR-100 (0–9) | CNN              | 0.6900   | 0.6884 |\n",
        "| CIFAR-100 (0–9) | ViT              | 0.6230   | 0.6266 |\n",
        "\n",
        "Full metrics, training time, parameter counts, and learning curves are saved in `results/`.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Insights\n",
        "- **MLPs perform well on tabular data**, where feature interactions are relatively simple.\n",
        "- **CNNs outperform MLPs on images** due to spatial inductive bias.\n",
        "- **Attention-based models require more data or pretraining** to outperform CNNs on images; without pretraining, ViT underperforms CNN on CIFAR.\n",
        "- On tabular data, **attention offers marginal gains** but does not drastically outperform simpler models.\n",
        "\n",
        "> Importantly, **I intentionally avoided pretraining**, as required by the assignment. Lower ViT performance is expected and correctly interpreted.\n",
        "\n",
        "---\n",
        "\n",
        "## Reproducibility\n",
        "1. Open the notebook in **Google Colab**\n",
        "2. Ensure **GPU is enabled**\n",
        "3. Run all cells one by one\n",
        "4. Run:\n",
        "```bash\n",
        "python main.py\n",
        "\n",
        "---\n",
        "```\n",
        "\n",
        "#Project Structure\n",
        "\n",
        "```\n",
        "shairamanandhar_assignment1_final/\n",
        "├── models/            # Architectures\n",
        "├── data_loaders/      # Dataset loaders\n",
        "├── config/            # Config file\n",
        "├── results/           # Metrics, plots, checkpoints\n",
        "├── main.py            # Training + evaluation\n",
        "└── README.md\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBulhxkB7yEW",
        "outputId": "40c3f933-b66d-46bb-e011-de1337c7155c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting README.md\n"
          ]
        }
      ]
    }
  ]
}